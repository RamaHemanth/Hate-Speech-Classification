{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06b94db4",
      "metadata": {
        "id": "06b94db4"
      },
      "source": [
        "#                                     TASK -- 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba4d3c1",
      "metadata": {
        "id": "fba4d3c1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf59f356",
      "metadata": {
        "id": "bf59f356",
        "outputId": "70851eec-658d-4baf-f59d-512e7ea2b9ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>we cannot continue calling ourselves feminists...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nawt yall niggers ignoring me</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>and this is why i end up with nigger trainee d...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nogs jews and dykes how enriching</td>\n",
              "      <td>offensive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text        type\n",
              "0  we cannot continue calling ourselves feminists...      normal\n",
              "1                      nawt yall niggers ignoring me      normal\n",
              "2  <user> i am bit confused coz chinese ppl can n...  hatespeech\n",
              "3  and this is why i end up with nigger trainee d...  hatespeech\n",
              "4                  nogs jews and dykes how enriching   offensive"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"NLP_ass_train.tsv\", sep='\\t',header=None)\n",
        "df.columns = ['text', 'type']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472fe5e0",
      "metadata": {
        "id": "472fe5e0",
        "outputId": "16dee380-1ccb-488e-99b1-12fe4b289af7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>this bitch in whataburger eating a burger with...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tired i can not support abortion from a moral ...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>all of my exes were cute but they were hoes i ...</td>\n",
              "      <td>offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; they visited helped provided foo...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it funny how people offended by the word nigge...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text        type\n",
              "0  this bitch in whataburger eating a burger with...  hatespeech\n",
              "1  tired i can not support abortion from a moral ...  hatespeech\n",
              "2  all of my exes were cute but they were hoes i ...   offensive\n",
              "3  <user> <user> they visited helped provided foo...      normal\n",
              "4  it funny how people offended by the word nigge...      normal"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_val = pd.read_csv(\"NLP_ass_valid.tsv\", sep='\\t',header=None)\n",
        "df_val.columns = ['text', 'type']\n",
        "df_val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6116d52",
      "metadata": {
        "id": "d6116d52",
        "outputId": "7a646cbc-8dbb-438a-98c9-3687476c7f87"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i dont think im getting my baby them white 9 h...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>laura loomer raped me while screaming at me in...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;user&gt; what did the old lady do was she a nazi...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>as much as i appreciate a format of open discu...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sex be so good a bitch be slow stroking and cr...</td>\n",
              "      <td>offensive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text        type\n",
              "0  i dont think im getting my baby them white 9 h...      normal\n",
              "1  laura loomer raped me while screaming at me in...  hatespeech\n",
              "2  <user> what did the old lady do was she a nazi...      normal\n",
              "3  as much as i appreciate a format of open discu...      normal\n",
              "4  sex be so good a bitch be slow stroking and cr...   offensive"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test = pd.read_csv(\"NLP_ass_test.tsv\", sep='\\t',header=None)\n",
        "df_test.columns = ['text', 'type']\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7945e523",
      "metadata": {
        "id": "7945e523",
        "outputId": "0960b345-f546-416c-a771-473435b3e1aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cannot continu call ourselv feminist right wom...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nawt yall nigger ignor</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>user  bit confus coz chines ppl access twitte...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>whi end nigger traine doctor speak proper lack...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nog jew dyke enrich</td>\n",
              "      <td>offensive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text        type\n",
              "0  cannot continu call ourselv feminist right wom...      normal\n",
              "1                             nawt yall nigger ignor      normal\n",
              "2   user  bit confus coz chines ppl access twitte...  hatespeech\n",
              "3  whi end nigger traine doctor speak proper lack...  hatespeech\n",
              "4                                nog jew dyke enrich   offensive"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem  import  WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words=stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "snow_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "def preprocessor(text):\n",
        "    text=text.split()\n",
        "    text=[snow_stemmer.stem(w) for w in text]\n",
        "    text=[lemmatizer.lemmatize(w,pos=\"a\") for w in text]\n",
        "    text=[w  for w in text if not w in stop_words]\n",
        "    text=' '.join(text)\n",
        "    text = re.sub(r'[^A-Za-z1-9 ]', ' ', text)\n",
        "    return text\n",
        "df['text']=df['text'].apply(preprocessor)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53844454",
      "metadata": {
        "id": "53844454"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim import models\n",
        "\n",
        "word2vec_path = 'GoogleNews-vectors-negative300.bin'\n",
        "word2vec_model = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e76793",
      "metadata": {
        "id": "b8e76793"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def get_input_features(dataframe):\n",
        "    input_features = []\n",
        "    for text in dataframe:\n",
        "        # Initialize an empty array for the text\n",
        "        feature_vector = np.zeros((word2vec_model.vector_size,))\n",
        "        num_words = 0\n",
        "        for word in text:\n",
        "            if word in word2vec_model:\n",
        "                feature_vector = np.add(feature_vector, word2vec_model[word])\n",
        "                num_words += 1\n",
        "        # Average the word vectors to get the feature vector for the text\n",
        "        if num_words > 0:\n",
        "            feature_vector = np.divide(feature_vector, num_words)\n",
        "        input_features.append(feature_vector)\n",
        "\n",
        "    # Convert input features and target labels to PyTorch tensors\n",
        "    input_features = torch.tensor(input_features, dtype=torch.float32)\n",
        "    return input_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f99043",
      "metadata": {
        "id": "64f99043",
        "outputId": "d2bef692-baf8-46bf-fdee-94822cbaf956"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-eae811c0f717>:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
            "  input_features = torch.tensor(input_features, dtype=torch.float32)\n"
          ]
        }
      ],
      "source": [
        "input_features=get_input_features(df['text'])\n",
        "# target_labels=['normal','hatespeech','offensive']\n",
        "target_labels=df['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)\n",
        "target_labels = torch.tensor(target_labels, dtype=torch.long)\n",
        "\n",
        "# Number of target classes in the dataset\n",
        "num_classes = 3  # Specify the number of target classes\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "X_train, y_train =input_features, target_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402774ff",
      "metadata": {
        "id": "402774ff"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec28e35",
      "metadata": {
        "id": "6ec28e35",
        "outputId": "6d18da3f-6e6d-406c-a258-36b4ca856bcc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:02<00:00, 178.65it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 141.50it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.64it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.10it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.18it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.39it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.65it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 131.20it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.30it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.03it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.08it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.87it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 122.77it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.48it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.21it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.88it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.50it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.68it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.44it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.07it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.19it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.06it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.62it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.11it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.70it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.91it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.06it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.45it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.54it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.70it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.73it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.93it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.57it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.37it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.49it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.48it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.84it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.48it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.93it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.54it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.14it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.48it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.62it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.69it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.40it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.04it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.88it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.15it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.72it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.70it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.12it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.01it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.15it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.27it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 122.37it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.08it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.31it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.93it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.42it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.55it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.20it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.09it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.19it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.04it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 123.65it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.14it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.35it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.64it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.24it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 131.65it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.77it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.67it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.96it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.15it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.48it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.19it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.51it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.47it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.40it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.59it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.47it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 128.51it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.82it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.70it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.59it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 130.06it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.09it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.35it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 129.78it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.66it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 127.25it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.92it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 125.44it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.05it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.29it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.63it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 123.77it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 124.98it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.26it/s]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████| 481/481 [00:03<00:00, 126.26it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = word2vec_model.vector_size\n",
        "hidden_size1 = 128\n",
        "hidden_size2 = 64\n",
        "output_size = num_classes\n",
        "\n",
        "model = NeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "#         print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d10c643b",
      "metadata": {
        "id": "d10c643b",
        "outputId": "9866d396-5897-4084-c45a-7d8cb1b31c70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set Accuracy: 44.02%\n"
          ]
        }
      ],
      "source": [
        "X_val, y_val =df_val['text'],df_val['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)\n",
        "y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "X_val=get_input_features(X_val)\n",
        "# Evaluating the model on the validation set\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_val)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = torch.sum(predicted == y_val).item() / y_val.size(0)\n",
        "\n",
        "print(\"Test Set Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'best_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151b1a06",
      "metadata": {
        "id": "151b1a06"
      },
      "source": [
        "# Accuracy using test data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1779ead4",
      "metadata": {
        "id": "1779ead4",
        "outputId": "cd6d3906-c897-40a6-e869-ced7c4e6f9fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set Accuracy: 45.17%\n"
          ]
        }
      ],
      "source": [
        "X_test, y_test =df_test['text'],df_test['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "X_test=get_input_features(X_test)\n",
        "# Evaluating the model on the validation set\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = torch.sum(predicted == y_test).item() / y_test.size(0)\n",
        "\n",
        "print(\"Test Set Accuracy: {:.2f}%\".format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e90bb4",
      "metadata": {
        "id": "e1e90bb4"
      },
      "source": [
        "#                                              TASK -- 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d8cd290",
      "metadata": {
        "id": "4d8cd290"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4b0781",
      "metadata": {
        "id": "8f4b0781"
      },
      "outputs": [],
      "source": [
        "# Sample raw text data\n",
        "df_train1 = pd.read_csv(\"NLP_ass_train.tsv\", sep='\\t',header=None)\n",
        "df_train1.columns = ['text', 'type']\n",
        "raw_text_data = df_train1['text']  # Your raw text data goes here\n",
        "target_labels = df_train1['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)   # Your target labels go here\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove unnecessary symbols\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Preprocess the raw text data\n",
        "processed_data = [preprocess_text(text) for text in raw_text_data]\n",
        "\n",
        "# Step 2: Create Vocabulary\n",
        "vocab = set(word for sentence in processed_data for word in sentence)\n",
        "\n",
        "# Step 3: Define Maximum Sequence Length\n",
        "max_seq_length = 50  # You can adjust this based on your dataset and task requirements\n",
        "\n",
        "# Step 4: Tokenize and Pad/Truncate Sentences\n",
        "word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
        "word_to_idx['<pad>'] = 0\n",
        "\n",
        "def encode_sentence(sentence):\n",
        "    encoded_sentence = [word_to_idx.get(word, 0) for word in sentence]\n",
        "    # Pad or truncate the sentence to the maximum sequence length\n",
        "    encoded_sentence = encoded_sentence[:max_seq_length] + [0] * (max_seq_length - len(encoded_sentence))\n",
        "    return encoded_sentence\n",
        "\n",
        "encoded_data = [encode_sentence(sentence) for sentence in processed_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d6c760",
      "metadata": {
        "id": "c5d6c760"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 5: load train, validation, and test sets\n",
        "df_val1 = pd.read_csv(\"NLP_ass_valid.tsv\", sep='\\t',header=None)\n",
        "df_val1.columns = ['text', 'type']\n",
        "df_test1 = pd.read_csv(\"NLP_ass_test.tsv\", sep='\\t',header=None)\n",
        "df_test1.columns = ['text', 'type']\n",
        "df_val1['text']=[preprocess_text(text) for text in df_val1['text']]\n",
        "df_test1['text']=[preprocess_text(text) for text in df_test1['text']]\n",
        "X_train, y_train = encoded_data, target_labels\n",
        "X_val,y_val = [encode_sentence(sentence) for sentence in df_val1['text']],df_val1['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)\n",
        "X_test, y_test = [encode_sentence(sentence) for sentence in df_test1['text']],df_test1['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)\n",
        "\n",
        "# Step 6: Define Dataloaders\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d66f5500",
      "metadata": {
        "id": "d66f5500",
        "outputId": "04e4ec48-6054-4020-d0e0-5216631c131c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Validation Loss: 1.0940\n",
            "Epoch [2/50], Validation Loss: 1.0874\n",
            "Epoch [3/50], Validation Loss: 1.0815\n",
            "Epoch [4/50], Validation Loss: 1.0849\n",
            "Epoch [5/50], Validation Loss: 0.9122\n",
            "Epoch [6/50], Validation Loss: 0.8727\n",
            "Epoch [7/50], Validation Loss: 0.8640\n",
            "Epoch [8/50], Validation Loss: 0.8832\n",
            "Epoch [9/50], Validation Loss: 1.0561\n",
            "Epoch [10/50], Validation Loss: 1.0992\n",
            "Epoch [11/50], Validation Loss: 1.1737\n",
            "Epoch [12/50], Validation Loss: 1.2925\n",
            "Early stopping. Loading the best model weights.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 7: Define Model Class\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1, dropout_prob=0.2):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)  #embedding layer\n",
        "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)   #rnn layer\n",
        "        self.dropout = nn.Dropout(dropout_prob) # Setting dropout for reducing overfitting\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  #classification layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.rnn(x)\n",
        "        output = output[:, -1, :]  # Use the last hidden state\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Apply softmax activation for predictions\n",
        "        x = F.softmax(self.forward(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Step 8: Train the Model with Early Stopping\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Number of target classes in the dataset\n",
        "num_classes = 3  # Specify the number of target classes\n",
        "\n",
        "model = RNNClassifier(vocab_size=len(vocab) + 1, embed_size=100, hidden_size=128, output_size=num_classes, num_layers=2, dropout_prob=0.2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(50):  # You can adjust the number of epochs\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f'Epoch [{epoch+1}/50], Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'best_rnn_model.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping. Loading the best model weights.\")\n",
        "            break\n",
        "\n",
        "# Step 9: Load Best Model Weights\n",
        "model.load_state_dict(torch.load('best_rnn_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd84163",
      "metadata": {
        "id": "4dd84163",
        "outputId": "c4270df8-07d0-4814-f92e-89fc663078d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.8611, Test Accuracy: 60.81%\n"
          ]
        }
      ],
      "source": [
        "# Step 10: Evaluate Model on Test Set\n",
        "model.eval()\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        test_loss += criterion(outputs, labels).item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy = correct_predictions / len(y_test)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba8eb114",
      "metadata": {
        "id": "ba8eb114"
      },
      "source": [
        "# TASK-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13cd1b34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13cd1b34",
        "outputId": "3d9a9d9b-3724-44cb-c3e4-659e18b9d894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c643841e",
      "metadata": {
        "id": "c643841e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_se4-e50AnL",
        "outputId": "046376f7-e90a-4019-a085-1abb15015b74"
      },
      "id": "C_se4-e50AnL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2c048c6",
      "metadata": {
        "id": "d2c048c6"
      },
      "outputs": [],
      "source": [
        "num_classes=3\n",
        "# Sample raw text data\n",
        "df_train1 = pd.read_csv(\"NLP_ass_train.tsv\", sep='\\t',header=None)\n",
        "df_train1.columns = ['text', 'type']\n",
        "raw_text_data = df_train1['text']  # Your raw text data goes here\n",
        "target_labels = df_train1['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)   # Your target labels go here\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove unnecessary symbols\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Preprocess the raw text data\n",
        "processed_data = [preprocess_text(text) for text in raw_text_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6518c015",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6518c015",
        "outputId": "9cdccd6f-ebae-434a-df7a-ee7f599cd2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Load Model & Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba01de4",
      "metadata": {
        "id": "4ba01de4"
      },
      "outputs": [],
      "source": [
        "# Step 3: Dataloader Class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'text': self.texts[idx], 'label': self.labels[idx]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa58e7a0",
      "metadata": {
        "id": "fa58e7a0"
      },
      "outputs": [],
      "source": [
        "# Step 4: Create Model Class\n",
        "class CustomBERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes, dropout_prob=0.2):\n",
        "        super(CustomBERTClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs['pooler_output']\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffaabb11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffaabb11",
        "outputId": "6a6a869e-790f-495a-ee18-1c5f53fcadc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Train Loop, Optimizers, Schedulers, Loss Function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Step 5: load train, validation, and test sets\n",
        "df_val1 = pd.read_csv(\"NLP_ass_valid.tsv\", sep='\\t',header=None)\n",
        "df_val1.columns = ['text', 'type']\n",
        "df_test1 = pd.read_csv(\"NLP_ass_test.tsv\", sep='\\t',header=None)\n",
        "df_test1.columns = ['text', 'type']\n",
        "df_val1['text']=[preprocess_text(text) for text in df_val1['text']]\n",
        "df_test1['text']=[preprocess_text(text) for text in df_test1['text']]\n",
        "X_train, y_train = processed_data, target_labels\n",
        "X_val,y_val =df_val1['text'],df_val1['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)\n",
        "X_test, y_test = df_test1['text'],df_test1['type'].map( {'normal': 0, 'hatespeech': 1, 'offensive': 2} ).astype(int)\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=512)\n",
        "\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aadc4605",
      "metadata": {
        "id": "aadc4605"
      },
      "outputs": [],
      "source": [
        "# Step 7: Train the Model with Early Stopping\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(50):  # You can adjust the number of epochs\n",
        "    model.train()\n",
        "    # Inside the training loop\n",
        "    for batch in train_loader:\n",
        "        input_texts = batch['text']\n",
        "        labels = batch['label'].long()  # Convert labels to torch.long data type\n",
        "        inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_texts = batch['text']\n",
        "            labels = batch['label']\n",
        "            inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            val_loss += criterion(logits, labels).item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f'Epoch [{epoch+1}/50], Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'best_bert_model.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping. Loading the best model weights.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc451cae",
      "metadata": {
        "id": "cc451cae"
      },
      "outputs": [],
      "source": [
        "# Step 8: Load Best Model Weights\n",
        "model.load_state_dict(torch.load('best_bert_model.pth'))\n",
        "\n",
        "# Step 9: Evaluate Model on Test Set\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_texts = batch['text']\n",
        "        labels = batch['label']\n",
        "        inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        test_loss += criterion(logits, labels).item()\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy = correct_predictions / len(y_test)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2760fd49",
      "metadata": {
        "id": "2760fd49"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98845f5e",
      "metadata": {
        "id": "98845f5e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}